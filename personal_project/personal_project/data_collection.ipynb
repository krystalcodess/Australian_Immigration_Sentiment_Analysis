{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMUeTirkiWejAoFkyjNkheF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iPl1HeYnbMo9","executionInfo":{"status":"ok","timestamp":1723430367567,"user_tz":-600,"elapsed":9716,"user":{"displayName":"Krystal Nguyen","userId":"17504450457163321072"}},"outputId":"a14ff798-5b68-45db-a7d2-6017b6751346"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting selenium\n","  Downloading selenium-4.23.1-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.0.7)\n","Collecting trio~=0.17 (from selenium)\n","  Downloading trio-0.26.2-py3-none-any.whl.metadata (8.6 kB)\n","Collecting trio-websocket~=0.9 (from selenium)\n","  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n","Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.7.4)\n","Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n","Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n","Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.2.0)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n","Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.7)\n","Collecting outcome (from trio~=0.17->selenium)\n","  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n","Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n","  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n","Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n","Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n","  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n","Downloading selenium-4.23.1-py3-none-any.whl (9.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trio-0.26.2-py3-none-any.whl (475 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.0/476.0 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n","Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n","Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n","Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n","Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.23.1 trio-0.26.2 trio-websocket-0.11.1 wsproto-1.2.0\n"]}],"source":["pip install selenium"]},{"cell_type":"code","source":["import time\n","from selenium.webdriver.support import expected_conditions as EC\n","from selenium.webdriver.support.ui import WebDriverWait\n","from bs4 import BeautifulSoup\n","from selenium import webdriver\n","from selenium.webdriver.chrome.service import Service\n","from selenium.webdriver.chrome.options import Options\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.common.keys import Keys\n","from selenium.common.exceptions import NoSuchElementException, TimeoutException\n","from webdriver_manager.chrome import ChromeDriverManager\n","import pandas as pd\n","import json\n","import requests"],"metadata":{"id":"BL1EcNkCbRih"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qQtmjRGubTA5","executionInfo":{"status":"ok","timestamp":1723430645161,"user_tz":-600,"elapsed":19110,"user":{"displayName":"Krystal Nguyen","userId":"17504450457163321072"}},"outputId":"5901ab6f-31a4-46fa-ca82-d42763664a17"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3wNZ-8eKE9iQ"},"outputs":[],"source":["def get_credentials() -> dict:\n","    credentials = {}\n","    with open('credentials.txt') as f:\n","        for line in f.readlines():\n","            try:\n","                key, value = line.split(\": \")\n","                credentials[key] = value.rstrip(\" \\n\")\n","            except ValueError:\n","                print('Add your email, username, and password in credentials file')\n","                exit(0)\n","    return credentials\n","\n","class Twitterbot:\n","    def __init__(self, email, username, password):\n","        self.email = email\n","        self.username = username\n","        self.password = password\n","        chrome_options = Options()\n","\n","        # Define the path to the ChromeDriver\n","        self.service = Service(ChromeDriverManager().install())\n","\n","        # Initialize the WebDriver with the Service\n","        self.bot = webdriver.Chrome(service=self.service, options=chrome_options)\n","\n","    def login(self):\n","        bot = self.bot\n","        bot.get('https://twitter.com/i/flow/login')\n","        time.sleep(3)\n","        try:\n","            # Wait for the email input field to be present and enter the email\n","            email_input = WebDriverWait(bot, 10).until(\n","                EC.presence_of_element_located((By.NAME, 'text'))\n","            )\n","            email_input.send_keys(self.email)\n","            email_input.send_keys(Keys.RETURN)\n","            time.sleep(2)\n","\n","            # Check if username input is requested\n","            try:\n","                username_input = WebDriverWait(bot, 5).until(\n","                    EC.presence_of_element_located((By.NAME, 'text'))\n","                )\n","                username_input.send_keys(self.username)\n","                username_input.send_keys(Keys.RETURN)\n","                time.sleep(2)\n","            except TimeoutException:\n","                # Username input not requested, proceed to password\n","                pass\n","\n","            # Wait for the password input field to be present and enter the password\n","            password_input = WebDriverWait(bot, 10).until(\n","                EC.presence_of_element_located((By.NAME, 'password'))\n","            )\n","            password_input.send_keys(self.password)\n","            password_input.send_keys(Keys.RETURN)\n","            time.sleep(2)\n","        except (NoSuchElementException, TimeoutException) as e:\n","            print(f\"An error occurred during login: {e}\")\n","\n","    def search_and_collect_tweets(self, search_query):\n","        bot = self.bot\n","        bot.get(f'https://twitter.com/search?q={search_query}&src=typed_query')\n","        time.sleep(3)\n","        tweet_text_elements = []\n","\n","        # Scroll multiple times to load more tweets\n","        for _ in range(200):  # Further increased scroll times for more data\n","            bot.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n","            time.sleep(2)\n","\n","            html_content = bot.page_source\n","            soup = BeautifulSoup(html_content, 'html.parser')\n","\n","            # Find all elements with 'tweetText' and tweet dates\n","            tweet_text_elements.extend(soup.find_all('div', {'data-testid': 'tweetText'}))\n","\n","        # Collect tweet data\n","        tweet_data = []\n","        for tweet_text_element in tweet_text_elements:\n","            tweet_text = tweet_text_element.text.strip().replace(\"\\n\", \"\")\n","            tweet_date_element = tweet_text_element.find_parent('article').find('time')\n","            tweet_date = tweet_date_element['datetime'] if tweet_date_element else 'No Date'\n","            tweet_data.append({'text': tweet_text, 'date': tweet_date})\n","\n","        # Convert the tweet data to a DataFrame\n","        df = pd.DataFrame(tweet_data)\n","        return df\n","\n","if __name__ == \"__main__\":\n","    credentials = get_credentials()\n","    bot = Twitterbot(credentials['email'], credentials['username'], credentials['password'])\n","    bot.login()\n","\n","    search_queries = [\n","        \"485Visa Australia\",\n","        \"StudentVisa500 Australia\",\n","        \"AustraliaImmigration\",\n","        \"AustraliaVisa\",\n","        \"TemporaryGraduate Australia\",\n","        \"Migration Australia\",\n","        \"ImmigrationPolicy Australia\",\n","        \"VisaUpdate Australia\",\n","        \"VisaApplication Australia\",\n","        \"StudyInAustralia\",\n","        \"Australia student visa\",\n","        \"Australia migration news\",\n","        \"Australia visa updates\",\n","        \"485 visa changes\",\n","        \"Australia immigration policy\",\n","        \"international students Australia\",\n","        \"post study work visa Australia\",\n","        \"Australian visa news\",\n","        \"temporary graduate visa\",\n","        \"Australia visa applications\"\n","    ]\n","\n","    # Initialize an empty list to collect DataFrames\n","    all_tweets = []\n","\n","    # Collect DataFrames in the list\n","    for query in search_queries:\n","        tweet_df = bot.search_and_collect_tweets(query)\n","        all_tweets.append(tweet_df)\n","\n","    # Concatenate all DataFrames into one DataFrame\n","    all_tweets_df = pd.concat(all_tweets, ignore_index=True)\n","\n","    # Drop duplicate rows\n","    all_tweets_df = all_tweets_df.drop_duplicates()\n","\n","    # Save the concatenated DataFrame to a single CSV file\n","    all_tweets_df.to_csv(\"twitter_df.csv\", index=False)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E4Hb6_PoE9iR"},"outputs":[],"source":["# Base URL for the Guardian API\n","baseUrl = \"https://content.guardianapis.com/search?q=\"\n","\n","def getdata(searchString, content, page, guardian_api_key):\n","    url = baseUrl + searchString + '&from-date=' + fromDate + '&to-date=' + toDate + '&api-key=' + guardian_api_key + '&page-size=' + '50' + '&page=' + page + '&production-office=aus'\n","    response = requests.get(url)\n","    if response.status_code != 200:\n","        return [], []\n","    data = json.loads(response.content)\n","    if 'response' in data and 'results' in data['response']:\n","        results = data['response']['results']\n","        contents = [result[content] for result in results if content in result]\n","        dates = [result['webPublicationDate'] for result in results if 'webPublicationDate' in result]\n","        return contents, dates\n","    else:\n","        return [], []\n","\n","# Function to get credentials from the credentials.txt file\n","def get_credentials() -> dict:\n","    credentials = {}\n","    with open('credentials.txt') as f:\n","        for line in f.readlines():\n","            key, value = line.split(\": \")\n","            credentials[key.strip()] = value.strip().strip('\"')\n","    return credentials\n","\n","# Fetch credentials\n","credentials = get_credentials()\n","api_key_guardian = credentials.get('guardian_api_key')\n","\n","# Define the date range for the search results\n","fromDate = \"2020-01-01\"\n","toDate = datetime.now().strftime('%Y-%m-%d')\n","\n","# Initialize lists to store article titles and publication dates\n","article_titles = []\n","publication_dates = []\n","\n","# Define the search terms and the number of pages to fetch for each term\n","search_terms = [\n","        \"485Visa\",\n","        \"StudentVisa500\",\n","        \"Immigration\",\n","        \"Visa\",\n","        \"TemporaryGraduate\",\n","        \"Migration\",\n","        \"ImmigrationPolicy\",\n","        \"VisaUpdate\",\n","        \"VisaApplication\",\n","        \"StudyInAustralia\",\n","        \"Australia student visa\",\n","        \"Australia migration news\",\n","        \"Australia visa updates\",\n","        \"485 visa changes\",\n","        \"Australia immigration policy\",\n","        \"international students Australia\",\n","        \"post study work visa Australia\",\n","        \"Australian visa news\",\n","        \"temporary graduate visa\",\n","        \"Australia visa applications\"\n","]\n","pages_to_fetch = 10\n","\n","# Fetch data for each search term and each page\n","for term in search_terms:\n","    for i in range(1, pages_to_fetch + 1):\n","        titles, dates = getdata(term, \"webTitle\", str(i), api_key)\n","        article_titles += titles\n","        publication_dates += dates\n","\n","# Create a DataFrame from the collected data\n","df1 = pd.DataFrame({\n","    \"Title\": article_titles,\n","    \"Publication Date\": publication_dates\n","})\n","\n","df1 = df1.drop_duplicates()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K1Gj2fN9E9iR"},"outputs":[],"source":["# Define a function to check if a title contains relevant topics\n","def contains_relevant_topic(title, search_terms):\n","    for topic in search_terms:\n","        if topic.lower() in title.lower():\n","            return True\n","    return False\n","\n","# Create a new DataFrame to store relevant articles\n","relevant_articles = []\n","\n","# Iterate through each row in the DataFrame\n","for index, row in df1.iterrows():\n","    title = row['Title']\n","    # Check if the title contains a relevant topic and does not only contain hashtags\n","    if contains_relevant_topic(title, search_terms) and not title.startswith(\"#\"):\n","        relevant_articles.append(row)\n","\n","# Convert the list of relevant articles back to a DataFrame\n","filtered_df1 = pd.DataFrame(relevant_articles)\n","\n","# Display the filtered DataFrame\n","filtered_df1.to_csv('guardian_data.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bj8qZAlVE9iS"},"outputs":[],"source":["# # config\n","import random\n","\n","chrome_options = Options()\n","# chrome_options.add_argument(\"--headless\")\n","chrome_options.add_argument(\"--disable-gpu\")\n","chrome_options.add_argument(\"--window-size=1920x1080\")\n","\n","driver = webdriver.Chrome(options=chrome_options)\n","\n","search_queries = [\n","    \"Visa 485 Australia\",\n","    \"Student Visa 500 Australia\",\n","    \"Australia Immigration\",\n","    \"Australia Visa\",\n","    \"Temporary Graduate Australia\",\n","    \"Migration Australia\",\n","    \"Immigration Policy Australia\",\n","    \"Visa Update Australia\",\n","    \"Visa Application Australia\",\n","    \"StudyInAustralia\",\n","    \"Australia student visa\",\n","    \"Australia migration news\",\n","    \"Australia visa updates\",\n","    \"485 visa changes\",\n","    \"Australia immigration policy\",\n","    \"international students Australia\",\n","    \"post study work visa Australia\",\n","    \"Australian visa news\",\n","    \"temporary graduate visa\",\n","    \"Australia visa applications\",\n","]\n","\n","\n","def scroll_down_page(driver):\n","    random_sleep = random.randint(5, 9)\n","    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n","    i = 1\n","    while True:\n","        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n","        time.sleep(random_sleep)\n","        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n","        if new_height == last_height:\n","            break\n","        last_height = new_height\n","        i += 1\n","        if i > 10:\n","            break\n","\n","\n","posts = []\n","\n","for query in search_queries:\n","    driver.get(f\"https://www.reddit.com/search/?q={query}\")\n","\n","    scroll_down_page(driver)\n","\n","    time.sleep(10)\n","\n","    # get all search results\n","    reddit_feed = driver.find_element(By.TAG_NAME, \"reddit-feed\")\n","\n","    reddit_posts = reddit_feed.find_elements(\n","        By.XPATH, \"//faceplate-tracker[@role='article']\"\n","    )\n","\n","    for post in reddit_posts:\n","        try:\n","            data = post.find_element(By.XPATH, \".//a[@data-testid='post-title']\")\n","            author = post.get_attribute\n","            href = data.get_attribute(\"href\")\n","            id = href.split(\"/\")[-3]\n","            text_body = data.text\n","            time_created = post.find_element(By.TAG_NAME, \"time\").get_attribute(\n","                \"datetime\"\n","            )\n","            posts.append([id, href, text_body, time_created])\n","        except Exception as e:\n","            print(f\"Error processing query: {query}\")\n","            print(e)\n","            break\n","\n","    print(f\"Finished processing query: {query}\")\n","\n","df = pd.DataFrame(posts, columns=[\"post_id\", \"href\", \"text_body\", \"time_created\"])\n","df.to_csv(\"./data/reddit_posts.csv\")\n","\n","time.sleep(10)\n","\n","def click_load_more_comments(driver, max_loads=20):\n","    load_count = 0\n","    while load_count < max_loads:\n","        try:\n","\n","            load_more_button = driver.find_element(\n","                By.XPATH, \"//button[contains(., 'View more comments')]\"\n","            )\n","\n","            load_more_button.click()\n","\n","            time.sleep(3)\n","\n","            load_count += 1\n","\n","        except Exception as e:\n","            print(f\"Error loading more comments: {e}\")\n","            break\n","\n","\n","comments = []\n","for post in posts:\n","    driver.get(post[1])\n","\n","    print(f\"Processing post: {post[0]}\")\n","\n","    click_load_more_comments(driver=driver, max_loads=20)\n","\n","    comments_tree = driver.find_elements(By.TAG_NAME, \"shreddit-comment-tree\")\n","\n","    reddit_comments = driver.find_elements(By.XPATH, \"//shreddit-comment[@depth='0']\")\n","\n","    for comment in reddit_comments:\n","        try:\n","            post_id = post[0]\n","            user = comment.get_attribute(\"author\")\n","            print(comment.get_attribute(\"postid\"))\n","            text_body = comment.find_element(By.XPATH, \".//div[@slot='comment']\").text\n","            time_created = comment.find_element(By.TAG_NAME, \"time\").get_attribute(\n","                \"datetime\"\n","            )\n","        except Exception as e:\n","            continue\n","\n","        comments.append([post_id, user, text_body, time_created])\n","\n","comment_df = pd.DataFrame(\n","    comments, columns=[\"post_id\", \"user\", \"text_body\", \"time_created\"]\n",")\n","comment_df.to_csv(\"./data/reddit_comments.csv\")"]}]}